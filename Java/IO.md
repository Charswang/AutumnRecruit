### 主要关键点

- 5大IO模型的特点
- 零拷贝
- Netty
- IO多路复用的特点，以及在其他软件上的应用例如：<font color="orange">Redis、RabitMQ怎样使用的？</font>
- select、poll、epoll特点和区别
- Java的正常IO流的使用【像File类，Bufferedxx对象，字节流、字符流、输入流、输出流等】
- 网络传输支持、序列化...等

---

<font color="orange">[可参考]</font>(https://blog.csdn.net/u014453898/article/details/109811000)

#### 文件描述符fd会与一个打开的文件相对应。

### 5大IO模型

- 阻塞io
- 非阻塞io
- io复用
- 异步io
- 信号驱动io

---

### 基于NIO的开源框架Netty

> 一个将**Java NIO进行封装**，大大减低了Java NIO的使用难度。/  一个Java NIO技术的开源**异步事件**驱动的**网络编程框架**
>
> Netty更多是是偏向数据操作优化【数据操作优化指的就是更容易上手吗？】这样的概念。
>
> 如RPC、Zookeeper都是基于Netty构建的

【详情】：



---

### 阻塞/非阻塞 & 同步/非同步

***阻塞 / 非阻塞***：体现的是线程状态；【阻塞：线程不能去干其他的事情；非阻塞：线程可以去干其他的事情】

***同步 / 异步***：体现的消息通知机制；【同步：数据有没有准备好需要靠自己判断；异步：数据没有准备好，是通过任务完成之后通知的】

> BIO: 同步阻塞IO；提交一个任务，直到任务完成，该线程不能去做其他事情，直到任务完成才可以去做其他事情
>
> NIO: 同步非阻塞IO；提交一个任务，直到任务完成之前，该线程可以去做其他事情，但是要定期过来判断，任务是否已经完成.【这里说是非阻塞，但也是在没有读写任务的时候，如果有了，也同样会阻塞一段时间的】
>
> AIO：异步非阻塞IO；提交一个任务，直到任务完成之前，该线程可以去做其他事情，直到任务完成，收到通知。
>
> 异步阻塞IO ？？？

---

### BIO ==> Blocking IO     阻塞IO

传统BIO方式，服务器一次只能处理一个请求，处理过程中客户端处于阻塞状态，处理完或发生异常之后，才可以执行第二个请求，不适合高并发的情况。

伪异步方式：多线程方式；

> 虽然服务器端有伪异步，但是实际上accept()还是阻塞的。

BIO的重点问题不是是否使用了多线程，而是为什么accept()和read()方法会被阻塞。异步IO就是为了解决这样的并发性而产生的。

---

## <font color="orange">重要</font>

### NIO ==> Non-Blocking IO  非阻塞IO

> 弥补原来io的不足，提供高速的面向块的io
>
> 普通的io是面向流[来一个字节搞一个字节]，NIO是面向块【来一个块搞一个块】，且是非阻塞的。

#### 通道

**通道**是什么：

**通道**与流的不同之处：

**通道**的一些分类例子。



#### 缓冲区

**缓冲区**：将数据写入或者写出通道的时候，都得先写到缓冲区中。

缓冲区的实质是什么：

【**缓冲区的读取方式**】缓冲区的变量以及是如何在缓冲区中进行数据读取的：



==========================================================================

**<font color="lightblue">上边说的是单纯的NIO所包含的东西？不涉及IO多路复用的东西吗？</font>**

===========================================================================

**<font color="orange">这里的Java NIO 是New IO，这个NEW IO中有一部分是跟NIO(非阻塞IO)有关系的</font>**

> #### 选择器--【<font color="orange">底层还是用的IO多路复用技术</font>】== 这里的Selector是Java NIO中的吧。与同步非阻塞IO还是有不同的。
>
> 实际上应该还是，NIO在等待数据的时候是非阻塞的，在数据从内核空间复制到用户空间的时候NIO是阻塞的；但是IO多路复用在两个阶段都是阻塞的。【<font color="orange">select  poll  epoll系统调用都是阻塞的</font>】
>
> Java NIO 实现io多路复用的reactor模型，一个线程使用一个选择器，然后通过一个选择器用轮询方式监听多个channel，让一个线程处理多个事件。**非阻塞体现在**：当一个channel的io事件还没到达的时候，线程不会在当前channel中阻塞等待，而是**轮询监听的所有channel**，找到io事件已经到达的channel。
>
> 【这里说的是Select模式么？-看起来貌似是的；**poll**与**select**没有太大差别：主要是在：select有最大连接数限制而poll没有。因为它是基于链表来存储的；两者都是连接数越大性能越差。**epoll**(不使用轮询，而是红黑树)可以理解为event poll（是事件驱动的），性能不是连接数决定的，而是活跃的连接数决定的，活跃的连接数越多，则性能相比就越差，epoll连接数连接数有上线，但是很大，1G内存可以有10W连接，可以理解为无限制？】
>
> 

**创建线程和线程切换会有大量开销，因此使用一个线程会有更好的性能。**

**只有套接字channel【*像通过UDP/TCP读写网络中数据的channel*】才可以配置为非阻塞，如果是FileChannel是不可以的。**



## - 零拷贝技术

========================================================================

#### <font color="lightblue">零拷贝总参考</font>【https://zhuanlan.zhihu.com/p/83398714】

========================================================================

在java中，其NIO称为NEW IO  

<font color="lightblue">Java NIO 与 前面说的NIO不是一个东西吧，Java NIO是IO多路复用模型结合了NIO？</font>

### Java  NIO零拷贝 -- 【先了解下Linux的零拷贝问题】。目前应该是只有<font color="orange">NIO</font>和<font color="orange">Epoll</font>方式有用到零拷贝技术。

> **通道**相当于内核空间中的缓冲区【既可以是读缓冲区也可以是Socket缓冲区】；**缓冲区**相当于用户空间的用户缓冲区【分为堆内存-HeapMap 和 堆外内存-DirectMap】；

- **MappedByteBuffer**

> 是NIO基于mmap-write/内存映射实现的
>
> 基于NIO中ByteBuffer缓冲区实现的
>
> 可不可以说是与NIO中的FileChannel和Byteuffer有些关系？

主要通过**FileChannel中的map方法是映射方法**，通过文件访问模式、获取数据的起始位置、以及数据大小；与虚拟内存中的数据进行映射。从而获取缓冲区的数据。

**map方法的底层原理**：首先是通过本地方法**map0()**为文件分配一块虚拟内存(堆外内存)，作为内存映射区域。map0方法返回内存映射区域的起始地址。然后通过Util包下的newMappedByteBuffer【可读可写】/ newMappedByteBufferR【只读】方法通过反射创建一个**DirectByteBuffer**实例(是MappedByteBuffer的一个子类)，最后返回这个DirectByteBuffer实例。【这里完成的是linux零拷贝中的前两次上下文切换和一次DMA copy？这里只是读取文件，还没有通过网络向客户端发送数据】

**读操作**的话，就直接调用map方法获取数据，使用MappedByteBuffer对象接收，然后可以读取。【具体看参考链接中的MappedByteBuffer中read部分的源码】

**写操作**的话，就要直接使用MappedBytedBuffer的put方法将数据写入缓冲区，然后使用force方法强制i将缓冲区的内容强制写入到本地文件中。【具体看参考链接中的MappedByteBuffer中write部分的源码】

map0方法的实现是通过JNI（Java Native Interface）调用的C程序实现的。

**<font color="orange">特点</font>：**1、使用**堆外内存**作为虚拟内存，因此分配的内存大小不受JVM限制，但是也是有大小限制的；2、**处理大文件性能高**，但是打开的文件只有在**垃圾回收的时候才会被关闭**，时间不确定，但是可以手动使用clean方法释放文件占用内存；4、当文件大小超过Integer.MAX_VALUE的话，可以通过position参数重新map/映射文件后面的内容。

- **DirectByteBuffer**

DirectByteBuffer在MappedByteBuffer的基础上**提供了对内存映像文件的随机读取和写入操作**

- **FileChannel**

  - 下面两个抽象方法通过在通道和通道之间建立连接实现数据传输。分别实现了**ReadableByteChannel**和**WriteableByteChannel**接口。

  - transferFrom()
  - transferTo()

transferFrom和transferTo底层都是用sendfile方式实现数据传输的。

**首先尝试sendfile方式**，如果操作系统不支持，**则尝试mmap内存映射方式**。这两种情况下目标通道不许是FileChannel或者SelChImpl类型。如果以上两种方式都失败，则使用**传统IO**进行读写。

============================================================================================

### Linux零拷贝【https://juejin.cn/post/6995519558475841550】

> 传输操作：将磁盘的文件读取出来，然后通过网络协议发送给客户端。
>
> 零拷贝是为了，在上述传输过程中，可以快速高效的将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间。

#### <font color="orange">零拷贝的动机</font>【https://zhuanlan.zhihu.com/p/266950886】

> 对于32位操作系统来说，它的寻址空间为4G(2的32次方)，其中前1G地址为内核空间，后3G地址为用户空间。
>
> 因为CPU所有指令中，有些指令是很危险的，如果错用会导致系统崩溃，比如清内存、设置时钟、读写文件等操作，如果允许所有程序都可以使用这些命令就会导致，系统崩溃的概率大大增加。因此在内核态下，进程运行在内核地址空间，CPU可以执行任何指令。用户态则，进程运行在用户地址空间，被执行的代码收到诸多CPU的检查。

因为所有的系统资源管理是在内核空间完成的，比如读写磁盘文件，从网络接口读取数据以及分配回收内存等操作。在读写文件时，程序需要先向内核发起一个“系统调用”，告诉内核：“我要读取磁盘文件”，**实际上就是通过一个特殊指令，让程序从用户态切换为内核态**，在内核态中，可以执行任何指令，具体就是，先把数据读取到内核空间，然后把数据拷贝到用户空间，并且要从内核态切换为用户态。**【这就导致了读取磁盘文件时会出现，<font color="orange">上下文切换和数据复制</font>的操作】**

- <font color="lightblue">读取磁盘文件的内核态与用户态的切换与数据复制。【4次复制，4次切换】--参考【https://zhuanlan.zhihu.com/p/266950886】</font>

  首先程序从用户态切换为内核态，内核态将磁盘数据读取并复制到内核空间的缓存区，然后从内核态切换为用户态，并将数据复制到用户空间的用户缓冲区。然后从用户态切换为内核态，并将数据复制到内核空间的Socket缓冲区，内核空间将数据复制并发送给网卡，网卡将数据发送出去。最后，再从内核态切换为用户态。【内核态一次是读取磁盘文件操作，一次是通过网络socket发送给客户端】

- 因此为了提升文件传输性能，需要减少【用户态与内核态的切换】和【内存拷贝】的次数。

内核空间：可以执行任何命令。

用户空间：会受到限制。

**零拷贝**

> 类似把内核空间和用户空间的虚拟地址映射到同一个物理地址，这样在IO操作时就不需要来回复制了？
>
> 因为多个虚拟地址可以执行同一个物理地址。虚拟内存的空间可以远大于物理内存空间。

**<font color="orange">零拷贝的主要目的是减少CPU copy和上下文切换的次数</font>**。【**几种技术**】

- ***mmap+write***  --  减少了一次cpu copy【不复制到用户态，也不从用户态复制到内核态，而是直接从内核态的页缓存复制到Socket缓冲区--这是一次cpu copy】，DMA复制是从磁盘复制到内核态和从内核态复制到网卡的操作。

  ***mmap==>文件映射内存***

图

- ***sendfile***  --  在mmap的基础上减少2次上下文切换的次数，减少应用程序发出的1次write/cpu copy操作。【是直接给内核发送命令吗，才没有了用户态和内核态的切换】--两次切换分别是：第一次用户态发起请求切换为内核态，第二次完成操作之后，由内核态切换为用户态？**是这样吗？**     另外的一次cpu copy是页缓存复制到Socket缓冲区的操作。

图

- ***带有scatter/gather的sendfile***  --  去除了所有的cpu copy，只剩下两次DMA copy【内核空间的Socker缓冲区直接指向页缓存的地址】 和 两次上下文切换。

图

- ***splice***  --  与sendfile不同的是splice不会需要硬件的支持。【sendfile是需要硬件和驱动的支持的】，可以说sendfile是splice的一个子集。【**没搞懂不需要硬件支持的原因**，但是下面说了是需要DMA的，DMA不是要再硬件之间复制数据吗，对，复制数据不是说要硬件支持。。。√】--splice利用linux管道机制，不使用cpu copy将内核缓冲区的数据传到Socket缓冲区中。还可以实现两个文件描述符中传输数据。

  DMA复制是依靠硬件完成的。*<font color="orange">DMA技术是负责**内存**和**其他组件**之间的数据拷贝。</font>*

|                                               | CPU拷贝 | DMA拷贝 |  系统调用  | 上下文切换 |
| :-------------------------------------------: | :-----: | :-----: | :--------: | :--------: |
|                   传统方式                    |    2    |    2    | read/write |     4      |
|   内存映射,[mmap是用户态切换为内核态的映射]   |    1    |    2    | mmap/write |     4      |
|                   sendfile                    |    1    |    2    |  sendfile  |     2      |
|         带有scatter/gatter的sendfile          |    0    |    2    |  sendfile  |     2      |
| <font color="red">splice【没弄懂！！】</font> |    0    |    2    |   splice   |     2      |

=======================================================================================

### **<font color="orange">RocketMQ的mmap与kafka的sendfile对比</font>**

- RocketMQ的mmap

  使用的非阻塞IO，基于多路复用处理，适用于小数据块/高频率的IO传输，大数据块会阻塞多路复用线程。

- kafka的sendfile

  kafka中的sendfile使用的是阻塞IO，适用于大数据块/低频率的IO传输。

---

### 信号驱动IO

> 预先告知内核，当某个描述符准备发生某件事情的时候，让内核发送一个信号【**SIGIO信号**】通知应用进程。
>
> 实际上应该就是，首先应用进程使用sigaction系统调用，内核立即返回，应用进程可以继续执行。然后内核时会等待数据，这个等待时间是非阻塞的，当内核的数据到达时向应用进程发送一个sigio信号，应用进程收到信号之后，调用recvfrom将数据从内核复制到应用进程中。

recvfrom()用于接收数据报/Socket传来的数据，并保存源地址。【要把recvfrom当作系统调用？？】

- 相比于非阻塞IO的轮询的方式，信号驱动IO的CPU利用率更高。【因为不用做无用的轮询？】

---

## <font color="red">还是不行，还是得重新再捋一遍</font>

### <font color="red">IO多路复用</font> --Java NIO中有用到，这里的Select   Poll  Epoll 都是同步阻塞的。

【参考】imageslr.com/2020/02/27/select-poll-epoll.html

> **一个线程具有处理多个I/O事件的能力；适合高并发场景**
>
> IO多路复用和NIO是两个相对独立的事情，NIO是指IO API总是能够立刻返回，不会被阻塞。IO多路复用是操作系统提供的一种便利的消息通知机制。
>
> <font color="orange">IO多路复用和NIO一般配合在一起使用才更有实际意义。</font>

以select复用方式为例：首先select监听多个套接字/文件描述符，轮询监听，等待一个或者多个内核中的数据准备好，准备好之后，来返回给应用进程一个可读信号【等待数据过程中也是阻塞的】，然后应用进程调用recvfrom将数据从内核复制到用户空间，数据复制过程中，线程也是阻塞的。【其中每个channel的访问是NIO-非阻塞IO，因此这样才可以轮询监听吧？】

[参考1](https://juejin.cn/post/6882984260672847879)

> **<font color="orange">select  poll  epoll  kqueue</font>**这些I/O模型的特点和区别。
>
> 多路是指：多个网络连接；复用是指：重复使用一个线程。
>
> 事件驱动IO

#### <font color="orange">select、poll效率低下的原因</font>;【zhuanlan.zhihu.com/p/187463036中的select缺点那里，<font color="red">都存在大量的文件描述符的换入换出</font>】

每执行一次select、poll、epoll都是一次系统调用，其中不确定的点是：**“select和poll是不是当其监听的所有文件描述符中，只要有一个就绪的就让用户空间遍历所有描述符来处理就绪的描述符，最后，再将描述符重新传给内核，继续监听所有文件描述符。这样来回复制文件描述符集合就会带来效率很低下，因此在监听的描述符很少的情况下啊，不影响效率，但是在坚挺的描述符很大，但是就绪的文件描述符很少的情况下就会使效率十分低下。”**

#### <font color="orange">epoll解决了上述问题，但不意味着所有情况下epoll都是最佳的调用方式</font>【<font color="red">pdai中Unix IO模型那里的select、poll、epoll的应用场景那里</font>】



#### <font color="orange">三种场景下的I/O多路复用机制【都是同步且阻塞的系统调用】--解释及原理</font>

> <font color="red">面试</font>
>
> select  poll  epoll  都是在内核态用的吗？
>
> 参考：
>
> <font color="lightblue">juejin.cn/post/6844904071187398663</font>
>
> <font color="lightblue">zhuanlan.zhihu.com/p/187463036</font>
>
> <font color="lightblue">imageslr.com/2020/02/27/select-poll-epoll.html</font>

#### - Select

select是一次系统调用，每次调用的方式：先将fd_set【fd_set的二进制的每一位作为一个文件描述符】的每一位都设置为0，然后将**想要监听的文件描述符的位置设置为1--真的假的，感觉不对劲**，然后调用select函数【用户态切换为内核态，将fd_set复制到内核空间】，然后遍历fd_set，看每个文件描述符是否就绪，如果没就绪，同时之前还设置成了1，那么要改成0，最后返回就绪的文件描述符个数。【【然后检测文件描述符集合的每一位是否为1？？】】

#### - Poll

与Select类似，性能开销与select相差不大。主要区别是poll能有更多的文件描述符。

#### - Epoll

优化select和poll，避免性能开销大和文件描述符数量少的问题。

“select中文件描述符数量少的问题：”Epoll使用红黑树存放文件描述符集合，可存放的数量较大。

“select&poll中开销较大--每次需要遍历所有文件描述符的问题：”Epoll使用队列作为就序列表来存放就绪的文件描述符。这样只需检查就序列表是否为空即可。由select的O(n)变为了O(1)；**<font color="red">每次调用不需要将文件描述符集合复制到内核</font>，select是要复制的，因为select是在用户态维护的文件描述集合，而epoll是在内核维护的，只要将新创建的文件描述符传入即可。**

回调机制？？

> epoll是在内核维护的文件描述符集合，只遍历发生事件的fd集合。【参考<font color="lightblue">juejin.cn/post/6844904071187398663</font>】
>
> - epoll_create：创建一个epoll对象【分配资源，创建红黑树和就绪链表】
> - epoll_ctl：传入一次文件描述符集合？添加到红黑树，然后向内核注册回调函数，当中断事件来临时将相应的就绪fd插入就绪链表，如果每次有新的创建的fd，就直接调用epoll_ctl进行添加即可，有修改或者删除的也是直接调用epoll_ctl来执行。【缓解了select、poll频繁换入换出文件描述符集合的问题，和文件描述符有限制的问题】
> - epoll_wait：相当于select--返回链表中的数据。【遍历就绪链表，缓解了select、poll每次轮询都是轮询整个文件描述符集合的效率问题】



<font color="orange">I/O多路复用的Epoll工作模式的两种--解释及原理 ====>  LT & ET</font>

参考：juejin.cn/post/6844904071187398663

> Epoll的描述符事件的两种触发方式：LT（Level Trigger）和 ET（Edge Trigger）
>
> LT是默认方式/水平触发。ET是高速模式/边缘触发

LT：支持Bolcking，也支持Non-Blocking；***如果用户由没处理的就绪的文件描述符，内核会再次将我未处理的就绪描述符加入到就绪队列中，不断通知用户***

ET：只支持Non-Blocking；***就绪描述符只通知用户一次，如果用户没做处理，内核将不再进行通知。***这样的效率更高，可以减少事件被重读触发的次数

***<font color="orange">ET必须使用NIO的原因</font>***：参考【<font color="lightblue">imageslr.com/2020/02/27/select-poll-epoll.html</font>】。

> 因为ET模式下，就绪的fd只会被通知一次，那么就要一次获取到一个fd的所有数据【使用循环read/write--while(input.read()!=null)】，那么如果使用的阻塞IO的话，必定会在最后一次读取的时候阻塞，导致无法正常结束【最后一次读取的时候，读取的肯定是null，但是如果是null的话，阻塞IO就会一直等待。】

**IO多路复用的出现的原因/动机**

> IO多路复用的最大优势是系统开销小，系统不必创建多余的线程，也不必维护，因此大大减少了系统开销。
>
> 优势不是并不是单个连接能处理的更快，而是在单个线程中能有更多的连接。



---

### <font color="lightblue">NIO与IO多路复用啥关系到底？</font>

> <font color="orange">主要参考：</font>https://blog.csdn.net/u014453898/article/details/109811000

Java NIO与NIO（同步非阻塞IO）还是有些不同的。--上面的参考【Java NIO是New IO，而NIO是Non-Blocking IO】

**<font color="lightblue">Java NIO(NEW IO)的NIO(Non-Blocking IO)的非阻塞</font>**体现在：当一个channel的io事件还没到达的时候，**线程不会在当前channel中阻塞等待，而是轮询监听的所有channel**，找到io事件已经到达的channel。如果是阻塞的话，就没有办法轮询检测其他channel了。

【这里的多路复用内部使用的NIO是只在内核态轮询，所以说IO多路复用的内部使用非阻塞IO与不适用多路复用只是用NIO的好处是：避免了NIO的那种频繁的用户态和内核态之间的频繁切换。】
